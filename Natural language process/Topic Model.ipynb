{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### she changlue\n",
    "20th April 2017\n",
    "\n",
    "This project is construct to handle documents topic classification.\n",
    "I use self designed algorithm to efficiently embedding the documents' topic vector and tokens' topic vector.It is emperiacally proved that this algorithm can classify the topic of different documents and tokens in a speed way.\n",
    "\n",
    "this notebook will process as follow:\n",
    "1. load library and raw corpus data\n",
    "2. cut the corpus in to a list format\n",
    "3. encode the tokens and corpus\n",
    "4. construct model and train\n",
    "5. use kmeans to do tokens' and docs' cluster \n",
    "6. use T-SNE to visualization\n",
    "7. save the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)   load library and raw corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custormTokens.txt\n",
      "opr_rem.csv\n",
      "催收sample.csv\n",
      "马上消费金融_jiuhui.wu_2017-04-05.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.cluster import KMeans#cluster\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba.posseg as pseg # cut the documents with token and tags\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE#cluster\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"corpus\"]).decode(\"utf8\"))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict('corpus/custormTokens.txt')  \n",
    "rawdata = pd.read_csv('corpus/opr_rem.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corpus briefing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opr_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今天下午5点472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\t告知客户电话15226088432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>说是叔侄关系  转告</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>客户承诺今天下午五点存入184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    opr_rem\n",
       "0                                 今天下午5点472\n",
       "1                       \\t告知客户电话15226088432\n",
       "2                               说是叔侄关系  转告 \n",
       "3                           客户承诺今天下午五点存入184\n",
       "4  已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999988, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opr_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>卡里有钱 450元,划扣，剩余的321明天下午五点之前存入银行卡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 opr_rem\n",
       "count                             999986\n",
       "unique                            999986\n",
       "top     卡里有钱 450元,划扣，剩余的321明天下午五点之前存入银行卡\n",
       "freq                                   1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) cut the corpus in to a list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenCorpus  = []#corpus list of cutted tokens\n",
    "rawSentences = []#raw text \n",
    "#documents    = list(rawdata[rawdata['消息目标']=='机器人']['消息内容'])#text which is send by custormers\n",
    "documents    = list(rawdata['opr_rem'])#text which is send by custormers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cstruct the corpus\n",
    "for sentence in documents:\n",
    "    if len(str(sentence))>4:\n",
    "        sentence = sentence.replace('\\t','')\n",
    "        words = [pair.word for pair in pseg.lcut(sentence) if pair.flag in ['n','ns','vs','nv','v']]       \n",
    "        if len(words)>1:\n",
    "            tokenCorpus.append(words)\n",
    "            rawSentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenCorpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawSentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) encode the tokens and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HP_miniTokenFreq = 3 #minimal tokens frequency\n",
    "tokenCount = dict()  #token count\n",
    "token2code = dict()  #token to code\n",
    "code2token = ['inFreqTokens'] #code to token\n",
    "code       = 1       #code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transfer token into codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get token frequency\n",
    "for tokens in tokenCorpus:\n",
    "    for token in tokens:\n",
    "        tokenCount.setdefault(token,0)\n",
    "        tokenCount[token]+=1  \n",
    "#encode those tokens which have minumal frequency\n",
    "for token in tokenCount:\n",
    "    if tokenCount[token] > HP_miniTokenFreq:       \n",
    "        token2code[token] = code\n",
    "        code += 1\n",
    "        code2token.append(token)\n",
    "    else:\n",
    "        token2code[token] = 0\n",
    "#transfer the raw token corpus into encoded corpus\n",
    "codeCorpus = []\n",
    "for tokens in tokenCorpus:          \n",
    "    codeCorpus.append([token2code[token]for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) construct model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HP_topicNums = 8\n",
    "documNums = len(codeCorpus)\n",
    "tokenNums = len(code2token)\n",
    "print(\"the training documents num:\",documNums)\n",
    "print(\"the training tokens    num:\",tokenNums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dense list to sparse numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenDocMat = np.zeros(shape=(documNums,tokenNums),dtype=np.float32)\n",
    "for docIdx in range(documNums):\n",
    "    tokenDocMat[docIdx][codeCorpus[docIdx]]=1\n",
    "docTokenNum = tokenDocMat.sum(axis=1).reshape((-1,1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docIndxSpan = np.array(range(documNums),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding initial\n",
    "docum2topicEmbed  = tf.Variable(tf.random_uniform([documNums,HP_topicNums], -1.0, 1.0))\n",
    "token2topicWeight = tf.Variable(tf.random_uniform([HP_topicNums,tokenNums], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholder\n",
    "document_idx   = tf.placeholder(tf.int32, shape=[None])\n",
    "document_embed = tf.nn.embedding_lookup(docum2topicEmbed , document_idx) \n",
    "multiplier     = tf.placeholder(tf.float32, shape=[None,1])\n",
    "tokensInDoc    = tf.placeholder(tf.float32,shape=[None,tokenNums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forward \n",
    "with tf.name_scope(\"TM\"):\n",
    "    documentembed_Sfmx     = tf.nn.softmax(document_embed)\n",
    "    token2topicWeight_Sfmx = tf.nn.softmax(token2topicWeight,dim=0)\n",
    "    tokensInDoc_pred       = tf.nn.softmax(tf.matmul(documentembed_Sfmx,token2topicWeight_Sfmx))*multiplier                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use least squre loss to train model\n",
    "with tf.name_scope(\"loss\"):   \n",
    "    loss = tf.reduce_mean(tf.square(tokensInDoc - tokensInDoc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer   = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial and save\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start training\n",
    "n_epochs   = 100\n",
    "batch_size = 1000\n",
    "batch_num  = documNums//batch_size+1\n",
    " \n",
    "with tf.Session() as sess:    \n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(batch_num):\n",
    "            #feed rawdata\n",
    "            document_idx_batch = docIndxSpan [batch*batch_size:(batch+1)*batch_size]\n",
    "            multiplier_batch   = docTokenNum [batch*batch_size:(batch+1)*batch_size]\n",
    "            tokensInDoc_batch  = tokenDocMat [batch*batch_size:(batch+1)*batch_size]\n",
    "             \n",
    "            #training\n",
    "            sess.run(training_op, feed_dict={document_idx : document_idx_batch[:20000],\n",
    "                                             multiplier   : multiplier_batch[:20000],\n",
    "                                             tokensInDoc  : tokensInDoc_batch[:20000]})  \n",
    "        if epoch%100==0:\n",
    "            train_loss = loss.eval(feed_dict={document_idx : docIndxSpan,\n",
    "                                              multiplier   : docTokenNum,\n",
    "                                              tokensInDoc  : tokenDocMat})\n",
    "            print(train_loss)\n",
    "    save_path = saver.save(sess,\"tfsave/topic_model.ckpt\")\n",
    "    # get embedding\n",
    "    documMat = documentembed_Sfmx.eval(feed_dict={document_idx : docIndxSpan})\n",
    "    tokenMat = token2topicWeight_Sfmx.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documMat.sum(axis=1)[:5],tokenMat.sum(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show the rough topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topK = 3\n",
    "outDF = []\n",
    "keyWords = []\n",
    "keySentences = []\n",
    "for idx in range(HP_topicNums):    \n",
    "    ords = np.argsort(-tokenMat[idx])[:topK]\n",
    "    tmp = ['主题'+str(idx+1)]\n",
    "    tmp += [code2token[i]for i in ords]\n",
    "    keyWords+=tmp\n",
    "    ords = np.argsort(-documMat.T[idx])[:topK]\n",
    "    tmp = ['------------------------------']\n",
    "    tmp +=[rawSentences[i]for i in ords]\n",
    "    keySentences+=tmp\n",
    "outDF = [keyWords,keySentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(outDF).T.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)  use kmeans to do tokens' and docs' cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenMat_KM = KMeans(n_clusters=HP_topicNums, random_state=0).fit(tokenMat.T)\n",
    "documMat_KM = KMeans(n_clusters=HP_topicNums, random_state=0).fit(documMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenMat_Label = tokenMat_KM.labels_\n",
    "documMat_Label = documMat_KM.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)  use T-SNE to visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenMat_tsne_embed = TSNE(random_state=1).fit_transform(tokenMat.T)  \n",
    "documMat_tsne_embed = TSNE(random_state=1).fit_transform(documMat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the visualization of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(tokenMat_tsne_embed[:,0],tokenMat_tsne_embed[:,1],c=tokenMat_Label, cmap=plt.cm.get_cmap(\"jet\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the visualization of document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(documMat_tsne_embed[:,0],documMat_tsne_embed[:,1],c=documMat_Label, cmap=plt.cm.get_cmap(\"jet\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)  save the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outDF= pd.DataFrame()\n",
    "outDF['text']=rawSentences\n",
    "outDF['label']=documMat_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outDF.to_csv('save/docLabel.csv',encoding='gbk',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
